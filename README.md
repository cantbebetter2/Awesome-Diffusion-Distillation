# Awesome-Diffusion-Distillation [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

This repo collects various distillation methods for the Diffusion model.  Welcome to PR the works (papers, repositories) missed by the repo. 

## Contents

* [Papers](#Papers)
  * [2023](#2023)
  * [2022](#2022)
  * [2021](#2021)

## Papers

### 2023

* [[ICML]](https://openreview.net/forum?id=bOVydU0XKC)On Architectural Compression of Text-to-Image Diffusion [[code]](https://github.com/Nota-NetsPresso/BK-SDM)![](https://img.shields.io/github/stars/Nota-NetsPresso/BK-SDM)
* [[ICML]](https://arxiv.org/abs/2303.01469v2)Consistency Models [[code]](https://github.com/openai/consistency_models)![](https://img.shields.io/github/stars/openai/consistency_models)
* [[ICML]](https://arxiv.org/abs/2308.06644)Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive [[code]](https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation)![](https://img.shields.io/github/stars/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation)
* [[ICML]](https://arxiv.org/abs/2307.05977)Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion [[code]](https://github.com/nannullna/safe-diffusion)![](https://img.shields.io/github/stars/nannullna/safe-diffusion)
* [[ICME]](https://ieeexplore.ieee.org/abstract/document/10219693)Accelerating Diffusion Sampling with Classifier-based Feature [[code]](https://github.com/tcl9876/Denoising_Student/tree/master)![](https://img.shields.io/github/stars/tcl9876/Denoising_Student)
* [[CVPR]](https://openaccess.thecvf.com/content/CVPR2023/html/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.html)On Distillation of Guided Diffusion [[code]](https://github.com/ruiqixu37/distill_diffusion)![](https://img.shields.io/github/stars/ruiqixu37/distill_diffusion)
* [[NeurIPs]](https://snap-research.github.io/SnapFusion/)SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds
* [[NeurIPs]](https://openreview.net/forum?id=MLIs5iRq4w)Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models
* [[PMLR]](https://proceedings.mlr.press/v202/zheng23d.html)Fast Sampling of Diffusion Models via Operator [[code]](https://github.com/neuraloperator/DSNO-pytorch)![](https://img.shields.io/github/stars/neuraloperator/DSNO-pytorch)
* [[arxiv]](https://arxiv.org/abs/2306.05544)BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping
* [[arxiv]](https://arxiv.org/abs/2303.04248)TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation
* [[arxiv]](https://arxiv.org/abs/2305.10769v4)Catch-Up Distillation: You Only Need to Train Once for Accelerating [[code]](https://github.com/shaoshitong/Catch-Up-Distillation)![](https://img.shields.io/github/stars/shaoshitong/Catch-Up-Distillation)
* [[arxiv]](https://arxiv.org/abs/2310.14189)Improved Techniques for Training Consistency [[code]](https://github.com/Kinyugo/consistency_models)![](https://img.shields.io/github/stars/Kinyugo/consistency_models)
* [[arxiv]](https://arxiv.org/abs/2311.17042v1)Adversarial Diffusion [[code]](https://github.com/Stability-AI/generative-models)![](https://img.shields.io/github/stars/Stability-AI/generative-models)

### 2022

* [[ICLR]](https://openreview.net/forum?id=TIdIXIpzhoI)Progressive Distillation for Fast Sampling of Diffusion [[code]](https://github.com/google-research/google-research/tree/master/diffusion_distillation)![](https://img.shields.io/github/stars/google-research/google-research)

### 2021

* [[arxiv]](https://arxiv.org/abs/2101.02388v1)Knowledge Distillation in Iterative Generative Models for Improved Sampling [[code]](https://github.com/tcl9876/Denoising_Student)![](https://img.shields.io/github/stars/tcl9876/Denoising_Student)

